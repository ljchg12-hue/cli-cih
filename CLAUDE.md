# CLAUDE.md v6.3.0 (Q&A + Error Handling + Agent Routing)

## Language Protocol
- Internal processing: English | User output: **Korean only**
- Exceptions: code blocks, technical terms, commands

## Mode Selection (Self-determine, never ask)
| Trigger | Mode | Action |
|---------|------|--------|
| Keywords: analyze/review/debug/fix/ë¶„ì„/ë¦¬ë·° | PRECISION | Full Q&A Loop â†’ AI CLI parallel |
| File path (no trigger keyword) | SIMPLE | Q&A Loop â†’ parallel tools |
| Questions/greetings only | CONVERSATION | Respond without tools |
| `/pipeline` or `l` | PIPELINE | Auto-chaining: ê¸°íšâ†’ê°œë°œâ†’í…ŒìŠ¤íŠ¸â†’ë¦¬ë·° |

### Pipeline Mode (prompt once before execution)
```
[íŒŒì´í”„ë¼ì¸ ëª¨ë“œ]
1. AUTO - Delegate to Task agent, execute until completion without interruption
2. STEP - Confirm after each phase
```
- **AUTO**: Delegate entire pipeline to Task(a:pipeline) agent â†’ auto-complete internally â†’ return only final result
- **STEP**: Existing method (confirm at each phase)
  - Confirmation prompt: `Phase N ì™„ë£Œ. ë‹¤ìŒ ì§„í–‰? (y/n/s)`
  - `y`: ë‹¤ìŒ Phase ì§„í–‰
  - `n`: í˜„ì¬ Phase ìˆ˜ì • ìš”ì²­
  - `s`: íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
- **Intervention**: "stop/ë©ˆì¶°/ì¤‘ë‹¨" â†’ abort agent â†’ report state

### Pipeline Auto-Suggestion (complexity detection)
Automatically add pipeline option to Q&A when:
- "ë§Œë“¤ì–´ì¤˜" + 3 or more features
- Project/system creation request
- Tasks expecting multiple file generation

## Q&A Loop / Protocol (SIMPLE/PRECISION modes)
**MANDATORY**: No modifying tools (Write/Edit/Bash) before "p"
- âœ… **í—ˆìš©**: Read, Grep, Glob (ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘)
- âŒ **ê¸ˆì§€**: Write, Edit, Bash (ìˆ˜ì • ì‘ì—…)
```
Required: PURPOSE / SCOPE / CONTEXT
Format: [ì§ˆë¬¸ N] + options (1-5) + commands (p/c/a/b/x/l)
```

### Commands
| Shortcut | Action |
|----------|--------|
| `p` | ì§„í–‰ (Proceed) |
| `c` | ì·¨ì†Œ (Cancel) |
| `a` | ì „ì²´ ê¸°ë³¸ê°’ ì ìš© |
| `b` | ì´ì „ìœ¼ë¡œ |
| `x` | ì¢…ë£Œ |
| `l` | **íŒŒì´í”„ë¼ì¸ ëª¨ë“œ** â†’ ìë™ ì²´ì´ë‹ ì‹¤í–‰ |

Display format:
```
(p:ì§„í–‰ / c:ì·¨ì†Œ / a:ì „ì²´ì ìš© / b:ì´ì „ / x:ì¢…ë£Œ / l:íŒŒì´í”„ë¼ì¸)
```

### Pipeline Option Display (when complexity is high)
```
[ì§ˆë¬¸ N] ì‘ì—… ë°©ì‹
1. ë‹¨ê³„ë³„ ì§„í–‰ (ì¼ë°˜)
2. íŒŒì´í”„ë¼ì¸ (ê¸°íšâ†’ê°œë°œâ†’í…ŒìŠ¤íŠ¸â†’ë¦¬ë·° ìë™) â† ê¶Œì¥
```

## Prohibited Actions
- Screenshot/browser automation without explicit request
- Background Bash processes > 2
- Kill Docker/Ollama/MCP servers
- Skip Q&A Loop for SIMPLE/PRECISION modes

## PRECISION Mode: AI CLI 3-Tier (after Q&A)
```bash
# Tier1 Cloud CLI 5ê°œ (Parallel - Required)
#   Claude(í˜„ì¬), Gemini, Codex, Copilot, GLM
#   â†’ cih_compareë¡œ ë³‘ë ¬ ì‹¤í–‰

# Tier2 Ollama Cloud S-Tier 4ê°œ (Parallel - Required)
#   mistral-large-3:675b-cloud, kimi-k2:1t-cloud
#   deepseek-v3.1:671b-cloud, cogito-2.1:671b-cloud
#   â†’ MCP ollama ë³‘ë ¬ ì‹¤í–‰

# Tier3 Local 2~4ê°œ (Sequential - Optional)
#   llama3.3, deepseek-r1:70b, ingu627/exaone4.0:32b(í•œêµ­ì–´)
#   â†’ VRAM ì‚¬ìš©í•˜ë¯€ë¡œ ìˆœì°¨ ì‹¤í–‰

# ì½”ë“œ Task ì¶”ê°€: qwen3-coder:480b-cloud, codellama:70b
# Total: Cloud 5 + Ollama Cloud 4 = 9ê°œ (Required)
#        + Local 2~4ê°œ (Optional) = 11~13ê°œ
```

### ğŸ”„ ì‹¤í–‰ ìˆœì„œ (Execution Order)
```
1ë‹¨ê³„: Cloud CLI 5ê°œ â†’ cih_compare (Parallel)
2ë‹¨ê³„: Ollama Cloud 4ê°œ â†’ MCP ollama (Parallel)
3ë‹¨ê³„: Ollama ë¡œì»¬ â†’ ìˆœì°¨ ì‹¤í–‰ (VRAM ì²´í¬ í•„ìˆ˜)
       â†’ nvidia-smi --query-gpu=memory.free
       â†’ 70B: 40GB / 32B: 20GB í•„ìš”
```

## TDD Workflow
RED (failing test) â†’ GREEN (minimal code) â†’ REFACTOR

## Stop Triggers
"stop", "ë©ˆì¶°", "ì¤‘ë‹¨", "cancel" â†’ Immediately halt all tool calls

## Error Handling
| ìƒí™© | ëŒ€ì‘ |
|------|------|
| AI CLI ì‘ë‹µ ì‹¤íŒ¨ | í•´ë‹¹ AI ìŠ¤í‚µ, ë‚˜ë¨¸ì§€ë¡œ ì§„í–‰ |
| MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨ | ì¬ì‹œë„ 1íšŒ â†’ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ì ì•Œë¦¼ |
| Tool í˜¸ì¶œ ì‹¤íŒ¨ | ì¬ì‹œë„ 2íšŒ â†’ ëŒ€ì•ˆ ë°©ë²• ì‹œë„ |
| ì „ì²´ ì‹¤íŒ¨ | í˜„ì¬ê¹Œì§€ ê²°ê³¼ ë³´ê³  + ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ |

## MCP Servers (cli-cih)

### Tools
| Tool | ìš©ë„ | ë¹„ê³  |
|------|------|------|
| `cih_quick` | ë‹¨ì¼ AI ë¹ ë¥¸ ì‘ë‹µ | default: ollama |
| `cih_compare` | ë©€í‹° AI ë¹„êµ | ë³‘ë ¬ ì‹¤í–‰ |
| `cih_discuss` | ë©€í‹° AI í† ë¡  | í•©ì„± í¬í•¨ |
| `cih_status` | AI ìƒíƒœ í™•ì¸ | ì‚¬ìš© ê°€ëŠ¥ ì²´í¬ |
| `cih_smart` | íƒœìŠ¤í¬ë³„ ìë™ ë¼ìš°íŒ… | code/debug/research |
| `cih_models` | ëª¨ë¸ ëª©ë¡ ì¡°íšŒ | - |

### ëª…ë ¹ì–´ í˜•ì‹
```bash
# Cloud CLI (cih_compareë¡œ ë³‘ë ¬)
gemini -p "prompt"
codex exec "prompt" --skip-git-repo-check
copilot -p "prompt" --allow-all  # Node 24 í•„ìš”
cih glm "prompt"

# Ollama Cloud (MCP ollama)
ollama run model:tag "prompt"
```

## References
- AI CLI: `~/.local/bin/ai-cli/AI_CLI_RULES.md`
- Agents: `~/.claude/agents/` (ë¼ìš°íŒ…: `ROUTING.md`)
- Skills: `~/.claude/skills/`
- Pipeline: `~/.claude/pipeline/` (state, workspace, templates, history)
